{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebd5cb6",
   "metadata": {},
   "source": [
    "Understanding encoders is a key point to understand how transformers work. In this notebook, we will present what encoders are and why they are useful by themselves and inside more complex architectures like the famous transformer architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc8b8e",
   "metadata": {},
   "source": [
    "# I. How do encoders work ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c7e2a",
   "metadata": {},
   "source": [
    "###  1. What are encoders ? \n",
    "\n",
    "Encoders are neural-network which, once trained, will give a mathematical representation of an input as the output. This sentence alone is not really enough to actually understand what they are and why they are useful, so we'll use the example of words.\n",
    "\n",
    "For example, let's say that you enter the sentence \"the cat likes cheese\" into an encoder, what might want to get as an output is an embedding vector (also called embedding tensor/feature vector/feature tensor) of each of these words. Just like the human brain does, the encoder allows us to get more information than what the letters composing the word give us. \n",
    "\n",
    "In this example, the word \"cats\" will have an embedding vector that, if we look at our representation space, will probably be close to \"dog\", \"mouse\" and other animals. Probably even better than that, the feature vector may contain information about cats being carnivorous, having claws or usually being domesticated. But to be clear, all of this cannot be clearly read by a human. The only thing that we can do is to make a projection of these high-dimensional representations and visualize how close \"similar\" words are. The dimension of these vectors is defined in the architecture of the model. \n",
    "\n",
    "Even better than that, the embedding will contain the meaning of a word in the context of the sentence: in this example, the embedding will be influenced by the fact that the word \"cat\" is positioned before the verb \"to like\" in this sentence. It is how we can differenciate different meanings of the same word depending on context. For instance:\n",
    "- \"bank\" in \"river bank\" vs \"saving bank\" \n",
    "- \"apple\" in \"apple fruit\" vs \"Apple company\"\n",
    "\n",
    "This is done by the **self-attention mechanism**, a very important part of how encoders work. \n",
    "\n",
    "### 2. How does the self-attention mechanism in encoders work ? \n",
    "\n",
    "The **self-attention mechanism** is what makes encoders so powerful:\n",
    "\n",
    "1. **Query, Key, Value**: Each word creates three vectors - think of it like asking questions (Query), having an identity (Key), and containing information (Value).\n",
    "\n",
    "2. **Attention scores**: Each word \"looks at\" every other word in the sentence and decides how much attention to pay to each one.\n",
    "\n",
    "3. **Weighted combination**: The final representation of each word is a weighted combination of all words' values, where weights are determined by relevance.\n",
    "\n",
    "**Example**: In \"The cat that I saw yesterday was black\"\n",
    "- When processing \"cat\", the attention mechanism will focus heavily on \"black\" and \"saw\"\n",
    "- When processing \"black\", it will focus on \"cat\" to understand what is black\n",
    "\n",
    "This is what allows the model to understand long-range dependencies and complex relationships within the sentence.\n",
    "\n",
    "### 3. In what case are encoders useful ? \n",
    "\n",
    "Even though we are often talking about encoders in the context of transformers, they can actually be used as standalone models in a variety of tasks:\n",
    "- Sequence classification (The mini-project of this repo will focus on this task, with a sentiment analysis model)\n",
    "- Masked language learning\n",
    "- Question answering\n",
    "- ...\n",
    "\n",
    "In general, encoders are useful for tasks linked to the need of a bi-directional extraction of meaningful information in a sequence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ef6bf",
   "metadata": {},
   "source": [
    "# II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6daa43",
   "metadata": {},
   "source": [
    "Even though the concept of getting \"embeddings\" of objects, i.e. getting a representation of them is old and even though the concept of encoders in neural networks dates back to earlier seq2seq models (2014-2015), the **Transformer encoder architecture** was introduced in \"Attention is All You Need\" (2017), revolutionizing how encoders work by replacing recurrent connections with pure self-attention mechanisms. \n",
    "\n",
    "This is the implementation that we will see in this notebook.\n",
    "\n",
    "Here is what an encoder looks like, based on the original paper:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/aiayn-encoder-scheme.png\" alt=\"Architecture\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Let's delve into each of its component:\n",
    "\n",
    "## 1. Multi-Head Attention\n",
    "\n",
    "**Multi-Head Attention** is by far the most complex component of the encoder/decoder in the transformer architecture.\n",
    "\n",
    "To understand Multi-Head Attention, we first need to know about **Scaled Dot-Product Attention**\n",
    "\n",
    "### 1.1. Scaled Dot-Product Attention\n",
    "\n",
    "Given a sequence of words: \n",
    "\n",
    "\"\\<bos\\> the green frog ran accross the river \\<eos\\>\"\n",
    "\n",
    "the goal is to find the relationship betweend these words $w_i$ where $i \\in \\mathbb{N}, i \\leq T$, $T$ being the length of the sequence.\n",
    "\n",
    "For each of this word, we will extract features $x_i$, and for each of these features, we extract three vectors $q_i, k_i, v_i$ corresponding to \"Query\", \"Key\" and \"Value\" vectors. These computations are based on a matrix multiplication:\n",
    "\n",
    "$q_i = x_i W^q_{d \\times d}$\n",
    "\n",
    "$k_i = x_i W^k_{d \\times d}$\n",
    "\n",
    "$v_i = x_i W^v_{d \\times d}$\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/attention_youtube_channel_vectors.png\" alt=\"Scheme\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "Now that we introduced the input, we can focus on the diagram of the Scaled Dot-Product Attention:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/aiayn-scaleddotproductattention-diagram.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "\n",
    "For all the mathematical details, check the video \"A Dive Into Multihead Attention, Self-Attention and Cross-Attention\" by Machine Learning Studio on Youtube.\n",
    "\n",
    "The \"Mask\" is optional and only useful to train model to generate new sequences (sequence to sequence). This mask can be applied to make the upper triangular part of the \"compatibility matrix\" (i.e. the matrix made by the matmul step) equal to $-\\infty$, to ensure that a word will not be able to \"see\" words with a larger index. \n",
    "\n",
    "The softmax step gives the attention weights. Note that at this step, $-\\infty$ becomes $0$.\n",
    "\n",
    "The last matmul between the attention weights and the value matrix give us the context matrix $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809a459",
   "metadata": {},
   "source": [
    "### 1.2. Multi-Head Attention\n",
    "\n",
    "**Multi-Head Attention** is the attention brick used within the transformer architecture, hence in the encoder architecture that we are implementing. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/aiayn-attentions.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Instead of performing a single attention on large matrixes Q, K and V, it is better to break it into multiple smaller dimensions and performe a scaled dot product separately on each of those smaller matrixes. \n",
    "\n",
    "1. First, we define how many heads we want to use for the multi-head attention\n",
    "\n",
    "2. Then, multiply $X$ with each weight matrices for $Q$, $K$ and $V$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/head_scheme.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "3. Then, we perform the same computation as seen before with Scaled Dot-Product Attention:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/scaled_dot_product_multihead.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "4. Then, we concatenate the result for each head to get the big \"usual\" matrix:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/concat_heads.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "5. Then, we finally go through a linear layer to get the output\n",
    "\n",
    "The computational cost is similar, but more efficient than a single Scaled Dot-Product Attention. The reason for that is that Multi head Attention can extract context information from different subspaces at different positions of the input sequence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30ff77",
   "metadata": {},
   "source": [
    "### 1.3. Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d2330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert (self.head_dim * num_heads == embed_dim), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.V = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.K = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self,\n",
    "                query,\n",
    "                keys,\n",
    "                values,\n",
    "                mask=None):\n",
    "        \n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        # 1. Extract the embeddings from the input:\n",
    "        Q = self.Q(query) # [N, query_len, embed_dim]\n",
    "        K = self.K(keys) # [N, key_len, embed_dim]\n",
    "        V = self.V(values) # [N, value_len, embed_dim]\n",
    "\n",
    "        # 2. Split embeddings into multiple heads\n",
    "        Queries = Q.reshape(N, query_len, self.num_heads, self.head_dim) # [N, query_len, num_heads, head_dim]\n",
    "        Keys = K.reshape(N, key_len, self.num_heads, self.head_dim) # [N, key_len, num_heads, head_dim]\n",
    "        Values = V.reshape(N, value_len, self.num_heads, self.head_dim) # [N, value_len, num_heads, head_dim]\n",
    "\n",
    "        # 3. Compute the attention scores\n",
    "        # matmul\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Queries, Keys])\n",
    "\n",
    "        # scale\n",
    "        energy = energy / (self.embed_dim ** (1/2)) # Explanations https://youtu.be/1IKrHh2X0F0?si=fQozjbfBRPw7J9p9\n",
    "        \n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        # apply softmax to get attention weights\n",
    "        attention = torch.softmax(energy, dim=3)\n",
    "\n",
    "        # final matmul between attention weights with values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, Values]).reshape(N, query_len, self.num_heads * self.head_dim) # [N, query_len, num_heads, head_dim]\n",
    "\n",
    "        # Out shape :       (N, query_len, num_heads, head_dim) after einsum and flattening the last two dimensions\n",
    "\n",
    "        # Final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        \n",
    "        return out \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962ff96",
   "metadata": {},
   "source": [
    "## 2. Transformer block\n",
    "\n",
    "Then, we are already ready to make the main block of the encoder architecture, and actualy of a whole transformer architecture:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/transformer_block.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Without going too far into details, it's understandable that skip connexions are used as transformers are derived from previous sequence models, with RNNs solving the problem of vanishing/exploding gradients first. If needed, check the theory of how residual connexions allow to have much deeper networks.\n",
    "\n",
    "The decoder can be split into two part:\n",
    "- As seen before, the first part is used for attention, which is extracting information of tokens in the specific context in which they are given in the input sequence. The layer normalization allows stabilization of training. The skip connection is also used as a stalizer as seen with the example of RNNs. That said, note that only the query is used for the residual connexion !\n",
    "\n",
    "- The second part is used to individually process the inputs, with the feed forward using an expansion, a ReLU layer on the expanded embedding to make more complex representation and a compression to go back to the original shape. Layer Normalization and skip connexion are also used for the same reason as before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80ea2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, forward_expansion * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query)) # Residual connection is done with just the query input\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ebfb8",
   "metadata": {},
   "source": [
    "## 3. Encoder\n",
    "\n",
    "Finally, the encoder architecture is done very fast since it's just one operation embedding extractions and a repetition of the TransformerBlock:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"ressources/encoder.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    num_heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion\n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef1e1c",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "\n",
    "\"Transformer: encoder\", Hugging Face Youtube channel (https://www.youtube.com/watch?v=MUqNwgPjJvQ)\n",
    "\n",
    "\"A Dive Into Multihead Attention, Self-Attention and Cross-Attention\", Machine Learning Studio Youtube channel (https://www.youtube.com/watch?v=mmzRYGCfTzc)\n",
    "\n",
    "\"Attention Is All You Need\", Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin(arXiv:1706.03762)\n",
    "\n",
    "\"Self-Attention Using Scaled Dot-Product Approach\", Machine Learning Studio Youtube channel (https://youtu.be/1IKrHh2X0F0?si=fQozjbfBRPw7J9p9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1003f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47467296",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
